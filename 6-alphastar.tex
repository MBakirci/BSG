\label{chapter:alphastar}

\subsection{About}
The goal of DeepMind is to build general artificial intelligence (AI). In order to benchmark an AI system, it is critical to test the performance of the agent in diverse scenarios. Gaming environments have become an important tool to test the capabilities of an agent. They provide clear definitions for winning and losing, their strategies can often be translated to general problems and any action taken within this environment does not affect the real-world.

Since the foundation of DeepMind in 2010, they have created several AI systems that reached a superhuman level of play. In 2003, DeepMind presented their AI system that was able to exceed all previous AI based approaches on six out of seven Atari 2600 games and outperformed a human expert on three of them \citep{mnih2013playing}. In March 2016, DeepMind’s AlphaGo won a game of Go against grandmaster Lee Sedol \citep{borowiec2016alphago}. 

The next step for DeepMind was to build an AI system for a more complex environment, StarCraft II. In cooperation with Blizzard Entertainment, DeepMind introduced the StarCraft II learning environment (SC2LE) \citep{vinyals2017starcraft}. Together with this environment, DeepMind has created a baseline agent and tested its results on several mini-games. However, the most interesting problem for them was to create an agent capable of playing a full competitive game of StarCraft II. This agent was named AlphaStar.

The official peer-reviewed paper regarding AlphaStar is not yet published. The information in this chapter is based DeepMind's blog post \citep{alphastarblog} and the papers it refers to. 

\subsection{Applied Methods and Techniques}
The applied methods and techniques in this section are categorized into the architecture and the training of AlphaStar. Because the official paper has not been released yet, any information about the motivation of the applied methods and techniques regarding AlphaStar is hypothetical. 

\subsubsection{The architecture of AlphaStar}~\\ 

As the authors of the blog post state: "AlphaStar's neural network architecture applies a transformer torso to the units \citep{vaswani2017attention}, combined with a deep LSTM core \citep{hochreiter1997long}, an auto-regressive policy head \citep{vinyals2017starcraft} with a pointer network \citep{vinyals2015pointer}, and a centralized value baseline \citep{alphastarblog}."

The transformer torso \citep{vaswani2017attention} is a sequence transduction model based solely on attention mechanisms. Traditional neural network architectures for solving sequence to sequence problems are based on recurrent layers. A critical constraint of this recurrent model is that it precludes parallelization since each layer conditions on the output of its previous layer. Another downside of this layer-per-layer approach is that the decoding layers have to travel across the whole network to access the first encoder's hidden state. The larger the path length, the more difficult it becomes to learn long-range dependencies \citep{hochreiter2001gradient}. The attention mechanisms in the Transformer allow the decoding layers to attend over every input layer directly. This method reduces the path length to a constant number and additionally allows parallelization. A game of StarCraft involves thousands of actions in which an early action's influence might only be noticeable  much later in the game, using a transformer makes it easier to learn these long-range dependencies. 

A pointer network \citep{vinyals2015pointer} is a neural architecture that allows the size of the output dictionary to be variable depending on the size of the input dictionary. The sequence-to-sequence \citep{sutskever2014sequence} and the input-attention model \citep{bahdanau2014neural} were used to form the baseline for this pointer network. The issue with these baseline models is that they require a fixed output size. The introduced pointer network solves this issue by using attention as a pointer to select a member of the input sequence as the output.

\subsubsection{The training of AlphaStar}~\\

The training of AlphaStar can be split into two stages. During the first stage AlphaStar’s neural network was trained by imitation learning from replays of human games. From these replays, AlphaStar was able to learn the basics of micro- and macromanagement strategies. This first version was able to beat the built-in “Elite” AI in 95\% of the games. In the second stage the DeepMind team used this first version to seed a multi-agent league in which a reinforcement learning process was applied. In each iteration of this league, new competitors were added by branching from existing competitors. These new agents typically play against competitors in proportion to the opponents win-rate. This prevents catastrophic forgetting, since the new agent must be able to beat all previous versions of itself. To discover unlikely strategies, DeepMind included a policy distillation cost to ensure that the agent continues to try human like behaviors with some probability. To encourage the agents to learn specialized strategies, each agent is given its own learning objective. The neural network weights of this agent are then updated by applying  reinforcement learning, to optimize its own learning objective. The authors of the AlphaStar blog describe this weight update rule as follows: 
"The weight update rule is an efficient and novel off-policy actor-critic \citep{espeholt2018impala} reinforcement learning algorithm with experience replay \citep{lin1992self}, self-imitation learning \citep{oh2018self} and policy distillation \citep{rusu2015policy}".

\subsection{Contributions to Reinforcement Learning}
AlphaStar is the first AI system that beat a professional human player in a RTS game. In December 2018 DeepMind invited teams Liquid's TLO and MaNa, two professional StarCraft players, to play against their AlphaStar in a Protoss versus Protoss matchup. AlphaStar won both matchups in a 5-0 series. Interesting to note is that AlphaStar used different strategies to win these games and some of these strategies were considered sub-optimal by pro players.

A point of criticism in terms of fairness in these games was that AlphaStar was able to see the entire map at once, while a human player has to move its camera to view specific areas. To address this, DeepMind trained a whole new agent that did not have this advantage. This new agent was put up against MaNa for a second time. In this game, MaNa actually copied one of the strategies AlphaStar used to beat him before. So while the initial version of AlphaStar was learning from human play, its final version was teaching professional players new strategies. However, MaNa was able find a weakness in this the newly created agent and exploited this weakness to win the game, making the total score of AlphaStar against professional players 10 to 1.

\subsection{Future plans}

The advanced neural network architecture of AlphaStar has proven to be capable of reaching a superhuman level of StarCraft II gameplay. To reach this level of play, AlphaStar heavily relied on the possibility for agents to play against each other in the AlphaStar league and evolve during the process. We think that it would be valuable to do more research on how this technique can be applied to real-world applications that do not have clear winning conditions, such as weather prediction. An interesting research question would be to explore the possibility of letting two weather prediction agents 'battle' against each other and determine the better agent. 