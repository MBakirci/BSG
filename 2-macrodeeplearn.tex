\label{chapter:macro-management}

\subsection{About}
All tasks related to planning and strategy are called macromanagement. This section discusses the findings of a research paper about the training of a neural network in macro management of StarCraft. The research presented a way to train a neural network which predicts macromanagement actions based on replays, which were live-action replays of expert human StarCraft players. Learning based on human actions can be described as imitation learning and the technique that is used by the authors of the paper, predicts actions based on games that are played by human players. Finally, the trained neural network has been put to action by applying it to an existing StarCraft bot: the UAlbertaBot. The UAlbertaBot is an open source, modular StarCraft bot\citep{churchill2015ualbertabot}. The modularity of the UAlbertaBot refers to the multiple agents within the bot with different tasks (e.g. macro management, micromanagement and pathfinding). These dedicated modules can be developed by anyone\citep{ontanon2013survey}. The module of the UAlbertaBot that will be swapped with the trained neural network is called the production manager. The production manager regulates the build order of troops and upgrades.

\subsection{Applied Methods and Techniques}

The neural network that is going to replace the standard production manager is trained based on the live-action replays. The training data consists of 2005 different matches between the Protoss and Terran race in StarCraft. From these matches, the following events were extracted: when units, upgrades or buildings are produced, when units or buildings are destroyed and when enemy units and buildings are observed. These events are used to simulate abstract StarCraft games via the build order forward model presented in Justesen and Risi\citep{justesen2017continual}. A build order forward model is a model which simulates the outcome of a build order, based on the current unit composition and available information about the opponent. The model is a predecessor of the macromanagement model which is presented in this paper, and favors short-term rewards over long-term rewards. The match-data is converted into state-action pairs by the StarCraft game engine, which resulted in a data set of 789,571 of such pairs. These were split up between a training set and a test set, in an 80:20 ratio respectively.

The neural network itself is a simple multi-layered network architecture with fully-connected layers. The problem is treated as a classification problem, in which the network tries to predict the next build given a game state. The output of the network is thus the probability of producing each build in a given state. The network is trained on the training set which is shuffled before each epoch. Xavier initialization is used for all weights in the hidden layers and biases are initialized to zero. Xavier initialization is an initialization process to assign network weights in such a way that the variance remains the same for a linear neuron\citep{glorot2010understanding}.
\begin{equation} \label{eq:xav}
y_i = x_iw_i + b_i
\end{equation}
The activation function of such an initialization is shown in equation (\ref{eq:xav}). In this equation $y_i$ is the argument vector, $x_i$ the activation vector and $w_i$ the weight at layer $i$.
Also, the learning rate is 0.0001 with the Adam optimization algorithm and a batch size of 100. The Adam optimization algorithm is an extension to stochastic gradient descent, which is used to update the network weights\citep{kingma2014adam}.

After training, the neural network is then applied to the UAlbertaBot instead of the standard production manager. The performance of the bot is split into two policies: greedy action and probabilistic action. The greedy action policy always selects the build with the highest probability. The probabilistic action policy selects builds with the probabilities of the softmax output units. The performance of the bot was tested in multiple 1 versus 1 - Protoss versus Terran - matches versus different in-game bots. The probabilistic action policy achieved a 59\% win rate against the built-in StarCraft bot when blind and a 68\% win rate when also receiving information about the opponent's material. The greedy strategy achieved a win rate of 53\% against the built-in bot. However, the neural network lost 100\% of all games played versus a bot with an powerful hand-designed rush strategy.

\subsection{Contributions to Reinforcement Learning}

According to Justesen and Risi\citep{justesen2017learning}, deep learning has never been used before to train a neural network to learn a specific function like macro management in a larger AI architecture. It is shown that macromanagement tasks can be learned from replays using deep learning, and that the learned policy can be used to outperform the built-in bot in StarCraft. The results were promising, but it did not lead to a bot that was able to deliver performance on-par with that of a human player.

One of the advantages is that by using state-action pairs, you can speed up the training enormously, since you can train the neural network on specific areas of interest. A problem with learning from replays, is that performance mostly depends on the quality of replays. A bot that is trained with imitation learning can never exceed the level of play displayed in the training set. It does not learn how to improve beyond that point.

\subsection{Future plans}

Further research would implement the trained neural network to a more sophisticated modular StarCraft bot that is capable of controlling multiple bases and more advanced units. Another plan for the future would be extending the current approach by applying regularization techniques (adding information to reduce overfitting) and including positional information of units and buildings. With these adjustments, the final plan for the future would be to improve this bot to compete with other high-performing StarCraft bots. 