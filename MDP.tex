\label{chapter:mdp}

\subsection{Basic idea behind the model}
People consider multiple times a day what actions they should and could take at a given moment. Before picking an action to perform the effects following the actions should be contemplated, these effects will be called rewards. Picking between actions with immediate rewards is easy because you can just pick the one with the best reward. But what if an action has a bad immediate reward but a better long term one. So the right arrangement between immediate rewards and future rewards has to be made.
\subsection{A standard Markov Decision Process}
With this idea in mind a Markov Decision Process (MDP) is created using four components:
\begin{enumerate}[nolistsep]
\item ${\displaystyle S}$ is a finite set of states
\item ${\displaystyle A}$ is a finite set of actions
\item ${\displaystyle P}$ is the effect the actions have on the state
\item ${\displaystyle R}$ is the immediate value of the actions
\end{enumerate}
\par
The state is the environment in which the agent operates, the actions the agent chooses to execute will transform the environment. A set of states is every possible variation the environment could be, each of these states would be a state in the MDP. The actions that are available to the agent depend on the scenario it is in, for example, the action \textit{open the door} would only be available if the agent is in a scenario where a door is available to be opened. For every scenario the agent is in, it has a set of possible actions it could pick from. The transition specifies how each actions modifies the state, this is something to consider when deciding between actions. The MDP will learn a policy, the policy specifies the best action to take for each of the states.
\subsection{Partial Observable Markov Decision Process}
The standard MDP is designed for an scenario where an agent has an complete view of the environment, however this is often not the case for a real world scenario where an agent has a scarce view of the environment. Performing actions on local observations and nothing more is desired in a scenario like this. Making an action based on the whole current state presents no problem when you have an complete view of the environment. But an partial view of the environment clouds the idea of the current state and what it looks like. To satisfy these new demands modifications were needed to the standard MDP. A Partial Observable Markov Decision Process (POMDP) was created with the same four component as a standard MDP, the only difference is whether or not the current state is observable. A new component was added:  ${\displaystyle \Omega}$ is a set of observations. These observations are provided by the state and can be used to get a idea about the current state. These observations can be probabilistic; so an observation model needs to be created. This model tells the probability of each observation for each state in the model.
\par
With the added partial observability the model no longer has direct access to the current state, the decisions condition on the entire history of the process. So an extra component is added: ${\displaystyle O}$ is a set of conditional observation probabilities. Instead of keeping track of the starting situation, all actions performed and all observations seen; the same information can be extracted from the probability distribution over all the states. 
\subsection{Decentralized partially observable Markov decision process}
What if there are two agents which a scare view of the environment. In the current model of POMDP the agents see each other as part of the state. To solve this multi-agent problem a new model was created called \textit{Decentralized partially observable Markov decision process} (Dec-POMDP). Instead of tracking the actions and observations of an individual agent, the joint actions and observations of all agents will be tracked. In a Dec-POMDP agents only know their own individual action and do not observe the actions of the other agents. Each agent acts on their own individual observations and no additional communication is assumed. The agents can however communicate through the state, if an agent needs to modify a property of the state the other agents will get the updated version of the state. In a Dec-POMDP at each stage each agent takes an action and receives local observations and a joint immediate reward. The observations they receive are individual observations but the reward generated by the environment is the same for all agents. The policy in a standard MDP specifies what the best action to take is, but since there are multiple agents who each have to take actions based on their own local observation; A local and joint policy is created.
